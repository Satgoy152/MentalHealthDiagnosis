{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_response import MentalHealthBot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer  # Changed from gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "class TooManyRequests(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MentalHealthRAGEvaluator:\n",
    "    def __init__(self, k=3, test_size=0.5, random_state=42):\n",
    "        self.bot = MentalHealthBot()\n",
    "        self.k = k\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.output_file = \"output-e5-large-v2.csv\"\n",
    "        # Load E5-large-v2 model\n",
    "        self.embedding_model = SentenceTransformer('intfloat/e5-large-v2')\n",
    "\n",
    "    def get_bot_response(self, prompt):\n",
    "        try:\n",
    "            response = self.bot.chat_stream(prompt)\n",
    "        except Exception as e:\n",
    "            raise TooManyRequests(\"API quota exceeded (429)\")\n",
    "        json_content = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        return json.loads(json_content.group()) if json_content else None\n",
    "\n",
    "    def _get_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        # Prepend \"query: \" to each text as recommended for E5 models\n",
    "        prefixed_texts = [\"query: \" + text for text in texts]\n",
    "        # Get embeddings from E5 model\n",
    "        return self.embedding_model.encode(prefixed_texts, normalize_embeddings=True)\n",
    "\n",
    "    def _save_response(self, text: str, label: str, response: dict):\n",
    "        data = {\n",
    "            'text': [text],\n",
    "            'true_label': [label],\n",
    "            'response': [json.dumps(response)],\n",
    "            'timestamp': [pd.Timestamp.now()],\n",
    "            'embedding_model': ['e5-large-v2']  # Updated model name\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        if os.path.exists(self.output_file):\n",
    "            df.to_csv(self.output_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(self.output_file, index=False)\n",
    "\n",
    "    def evaluate(self, data_path: str):\n",
    "        df = pd.read_csv(data_path).dropna()\n",
    "        train_df = df\n",
    "        test_df = pd.read_csv(\"Combined-Data.csv\").dropna()\n",
    "\n",
    "        # Sample 1000 random rows from test data\n",
    "        test_df = test_df.sample(n=1000, random_state=self.random_state).reset_index(drop=True)\n",
    "\n",
    "        train_texts = train_df.iloc[:, 0].tolist()\n",
    "        train_embeddings = self._get_embeddings(train_texts)\n",
    "\n",
    "        test_texts = test_df.iloc[:, 1].tolist()\n",
    "        test_embeddings = self._get_embeddings(test_texts)\n",
    "        test_labels = test_df.iloc[:, 2].tolist()\n",
    "\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.k).fit(train_embeddings)\n",
    "        distances, indices = self.nn.kneighbors(test_embeddings)\n",
    "\n",
    "        pred_labels = []\n",
    "        try:\n",
    "            for i, (text, label) in enumerate(zip(test_texts, test_labels)):\n",
    "                neighbor_labels = [train_df.iloc[j, 1] for j in indices[i]]\n",
    "                pred_label = max(set(neighbor_labels), key=neighbor_labels.count)\n",
    "                pred_labels.append(pred_label)\n",
    "\n",
    "                response = self.get_bot_response(f\"Analyze this mental health statement: {text}\")\n",
    "                if response:\n",
    "                    self._save_response(text, label, response)\n",
    "\n",
    "        except TooManyRequests:\n",
    "            print(\"Received 429 error. Exiting early to preserve existing results.\")\n",
    "\n",
    "        if len(pred_labels) < len(test_labels):\n",
    "            test_labels = test_labels[:len(pred_labels)]\n",
    "\n",
    "        accuracy = accuracy_score(test_labels, pred_labels)\n",
    "        f1 = f1_score(test_labels, pred_labels, average='weighted')\n",
    "        report = classification_report(test_labels, pred_labels)\n",
    "\n",
    "        unique_classes = sorted(set(test_labels))\n",
    "        per_class_acc = {}\n",
    "        for cls in unique_classes:\n",
    "            mask = np.array(test_labels) == cls\n",
    "            per_class_acc[cls] = accuracy_score(\n",
    "                np.array(test_labels)[mask],\n",
    "                np.array(pred_labels)[mask]\n",
    "            )\n",
    "\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Weighted F1 Score: {f1:.4f}\")\n",
    "        print(\"\\nPer-Class Accuracy:\")\n",
    "        for cls, acc in per_class_acc.items():\n",
    "            print(f\"- {cls}: {acc:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "\n",
    "        return {\n",
    "            'overall_accuracy': accuracy,\n",
    "            'weighted_f1_score': f1,\n",
    "            'per_class_accuracy': per_class_acc,\n",
    "            'classification_report': report,\n",
    "            'output_file': self.output_file\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MentalHealthRAGEvaluator(\n",
    "        k=3,\n",
    "        test_size=0.005,\n",
    "        random_state=42\n",
    "    )\n",
    "    results = evaluator.evaluate(\"Subset-Data.csv\")\n",
    "    print(\"Evaluation complete. Results saved to output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
