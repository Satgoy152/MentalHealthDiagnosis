{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_content(text):\n",
    "    start = text.find('{')  # Find the index of the first '['\n",
    "    end = text.rfind('}')   # Find the index of the last ']'\n",
    "\n",
    "    # Extract the JSON content between the first '[' and the last ']'\n",
    "    json_content = text[start:end + 1]\n",
    "\n",
    "    # Extract the JSON content\n",
    "    if start == -1 or end == -1:\n",
    "        raise ValueError(\"No JSON object found in the text.\")\n",
    "    return text[start:end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bot -----------------------------------###\n",
      "Initializing RAG system\n",
      "Initializing LLM\n",
      "Creating RAG chain\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     83\u001b[39m     evaluator = MentalHealthRAGEvaluator(k=\u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCombined-Data.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mMentalHealthRAGEvaluator.evaluate\u001b[39m\u001b[34m(self, data_path)\u001b[39m\n\u001b[32m     37\u001b[39m train_texts = train_df.iloc[:, \u001b[32m1\u001b[39m].tolist()\n\u001b[32m     38\u001b[39m train_labels = train_df.iloc[:, \u001b[32m2\u001b[39m].tolist()\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m X_train = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Nearest Neighbors\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.nn = NearestNeighbors(n_neighbors=\u001b[38;5;28mself\u001b[39m.k).fit(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1262\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1265\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m    109\u001b[39m             doc = ngrams(doc, stop_words)\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m             doc = \u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kitty\\OneDrive\\Desktop\\test_rave\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:238\u001b[39m, in \u001b[36m_VectorizerMixin._word_ngrams\u001b[39m\u001b[34m(self, tokens, stop_words)\u001b[39m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m         )\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_word_ngrams\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens, stop_words=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    239\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# handle stop words\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from llm_response import MentalHealthBot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import re\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "class MentalHealthRAGEvaluator:\n",
    "    def __init__(self, k=3, test_size=0.2, random_state=42):\n",
    "        self.bot = MentalHealthBot()\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.k = k\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    def get_bot_response(self, prompt):\n",
    "        try:\n",
    "            response = self.bot.chat_stream(prompt)\n",
    "            json_content = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            return json.loads(json_content.group()) if json_content else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting response: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate(self, data_path):\n",
    "        df = pd.read_csv(data_path).dropna()\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=self.test_size, random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # TF-IDF Vectorization\n",
    "        train_texts = train_df.iloc[:, 1].tolist()\n",
    "        train_labels = train_df.iloc[:, 2].tolist()\n",
    "        X_train = self.vectorizer.fit_transform(train_texts)\n",
    "        \n",
    "        # Nearest Neighbors\n",
    "        self.nn = NearestNeighbors(n_neighbors=self.k).fit(X_train)\n",
    "        \n",
    "        # Evaluation\n",
    "        y_true, y_pred = [], []\n",
    "        for _, row in test_df.head(500).iterrows():\n",
    "            query = row.iloc[1]\n",
    "            true_label = row.iloc[2].lower()\n",
    "            y_true.append(true_label)\n",
    "            \n",
    "            try:\n",
    "                # Find similar examples using TF-IDF\n",
    "                query_vec = self.vectorizer.transform([query])\n",
    "                _, indices = self.nn.kneighbors(query_vec)\n",
    "                \n",
    "                # Create prompt\n",
    "                prompt = \"Similar examples:\\n\"\n",
    "                for i in indices[0]:\n",
    "                    prompt += f\"- {train_texts[i][:150]}... [Label: {train_labels[i]}]\\n\"\n",
    "                prompt += f\"\\nClassify this:\\n{query[:300]}\\nCategory:\"\n",
    "                \n",
    "                # Get prediction\n",
    "                response = self.get_bot_response(prompt)\n",
    "                pred = (response.get('Category') or response.get('category')).lower()\n",
    "                y_pred.append(pred)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing: {str(e)}\")\n",
    "                y_pred.append(\"error\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        valid_indices = [i for i, pred in enumerate(y_pred) if pred != \"error\"]\n",
    "        if valid_indices:\n",
    "            y_true_valid = [y_true[i] for i in valid_indices]\n",
    "            y_pred_valid = [y_pred[i] for i in valid_indices]\n",
    "            \n",
    "            print(\"\\nEvaluation Metrics:\")\n",
    "            print(f\"Accuracy: {accuracy_score(y_true_valid, y_pred_valid):.4f}\")\n",
    "            print(f\"Macro F1: {f1_score(y_true_valid, y_pred_valid, average='macro'):.4f}\")\n",
    "            print(classification_report(y_true_valid, y_pred_valid, zero_division=0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = MentalHealthRAGEvaluator(k=5)\n",
    "    evaluator.evaluate(\"Combined-Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
